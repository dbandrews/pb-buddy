name: Complete Scrape

on:
 schedule:
   - cron: "23 7 * * 0"
 workflow_dispatch:
    inputs:
      num_jobs:
        description: "How many cores to use in building link list"
        required: true
        default: '4'
env:
  ACTIONS_ALLOW_UNSECURE_COMMANDS: true

jobs:
  scrape-latest:
    runs-on: self-hosted2
    timeout-minutes: 2000
    steps:
      - name: Setup repo
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.11
      - name: Install package
        shell: bash -l {0}
        run : |
          pip install poetry
          poetry install
      - name: Set env vars
        run: |
          echo "DATE=$(python -c 'import datetime as dt; print((dt.datetime.now()))')" >> $GITHUB_ENV
      - name: Log env
        run: env
      - name: Run Scraper
        shell: bash -l {0}
        run: python3 -m poetry run python scripts/scrape_base.py --full_refresh=True --num_jobs=${{github.event.inputs.num_jobs}}
        env: 
          COSMOS_CONN_STR : ${{secrets.COSMOS_CONN_STR}}
